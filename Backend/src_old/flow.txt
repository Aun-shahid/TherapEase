

we will keep the flow like this

ensure file is in wav form 

preprocess the file
    convert to wav
    truncate the empty parts (no voice) from the start or end

send the file in 30s segments to whisper

combine the result


send for speaker identification


return the diarized result as json


now some more instructions for speech to text:
    the conversation is in urdu but english words may be used, the words used in english should be recognized as english.



consideration: 
    finding a way to break down the wav file into multiple segments and labelled by the speaker name based on the speakers
    voice might be the most efficient way. then we can send each segment for transcription and we wont have to run a separate speaker recognition
    in this approach the speaker recognition will be before the transcription.
    _additionally we can call the speaker recognition again to refine the transcript if there was any mistake in the initial speaker recognition


# a way to get the pause between each sentence would be great, the ms delay between two words.
    > this would allow us to better distinguist between speakers by knowing when a sentence ended
    > _additionally this would help us in knowing when the patient took a pause, it might have some meaning.